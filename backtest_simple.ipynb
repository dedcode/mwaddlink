{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:30:08.669999Z",
     "start_time": "2020-09-10T03:30:08.657997Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import shelve\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import mwparserfromhell as mwph\n",
    "from mwparserfromhell.nodes.text import Text\n",
    "from mwparserfromhell.nodes.wikilink import Wikilink \n",
    "import wikitextparser as wtp\n",
    "\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import operator\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from scripts.utils import wtpGetLinkAnchor\n",
    "from scripts.utils_features import get_feature_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'simple'\n",
    "wiki   = lang+'wiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = shelve.open(\"./data/{0}/{0}.anchors.db\".format(lang),flag='r')\n",
    "pageids = shelve.open(\"./data/{0}/{0}.pageids.db\".format(lang),flag='r')\n",
    "redirects = shelve.open(\"./data/{0}/{0}.redirects.db\".format(lang),flag='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load word2vec features\n",
    "word2vec = shelve.open(\"./data/{0}/{0}.w2v.filtered.db\".format(lang), flag='r' )\n",
    "## load navigation-vector features\n",
    "nav2vec = shelve.open(\"./data/{0}/{0}.nav.filtered.db\".format(lang), flag='r' )\n",
    "\n",
    "## load trained model\n",
    "model = xgb.XGBClassifier()  # init model\n",
    "model.load_model('./data/{0}/{0}.linkmodel.bin'.format(lang))  # load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:30:32.061048Z",
     "start_time": "2020-09-10T03:30:32.037436Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the sentences to test\n",
    "test_set = []\n",
    "with open('./data/{0}/training/sentences_test.csv'.format(lang)) as fin:\n",
    "    for line in fin:\n",
    "        try:\n",
    "            title, sent = line.split('\\t')\n",
    "            test_set.append((title, sent))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:31:40.123839Z",
     "start_time": "2020-09-10T03:31:40.109073Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main decision function.\n",
    "\n",
    "# for a given page X and a piece of text \"lipsum\".. check all the candidate and make inference\n",
    "# Returns the most likely candidate according to the pre-trained link model\n",
    "# If the probability is below a certain threshold, return None\n",
    "def classify_links(page, text, THRESHOLD):\n",
    "    #start_time = time.time()\n",
    "    cand_prediction = {}\n",
    "    # Work with the 10 most frequent candidates\n",
    "    limited_cands = anchors[text]\n",
    "    if len(limited_cands) > 10:\n",
    "        limited_cands = dict(sorted(anchors[text].items(), key = operator.itemgetter(1), reverse = True)[:10]) \n",
    "    for cand in limited_cands:\n",
    "        # get the features\n",
    "#         cand_feats = get_feature_set(page, text, cand, anchors, word2vec,nav2vec,pageids)\n",
    "        cand_feats = get_feature_set(page, text, cand, anchors, word2vec,nav2vec)\n",
    "\n",
    "        # compute the model probability\n",
    "        cand_prediction[cand] = model.predict_proba(np.array(cand_feats).reshape((1,-1)))[0,1]\n",
    "    \n",
    "    # Compute the top candidate\n",
    "    top_candidate = max(cand_prediction.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    # Check if the max probability meets the threshold before returning\n",
    "    if top_candidate[1] < THRESHOLD:\n",
    "        return None\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return top_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:31:47.643912Z",
     "start_time": "2020-09-10T03:31:47.631050Z"
    }
   },
   "outputs": [],
   "source": [
    "# Article parsing utility.\n",
    "\n",
    "# For a given page return the list of all existing links and mentions\n",
    "# To avoid linking what's already linked\n",
    "# Article parsing utility.\n",
    "def getLinks(wikicode, page_title):\n",
    "    m = set()\n",
    "    e = set()\n",
    "    page_title_tmp = page_title.replace('_',' ')\n",
    "    # add the page title itself\n",
    "    m.add(page_title_tmp)\n",
    "    e.add(page_title_tmp)\n",
    "    linklist = wtp.parse(str(wikicode)).wikilinks\n",
    "    for l in linklist:\n",
    "        link,anchor = wtpGetLinkAnchor(l)\n",
    "        m.add(anchor)\n",
    "        e.add(link)\n",
    "#         m.add(l.plain_text().strip())\n",
    "#         e.add(l.title.strip())\n",
    "    return m, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article parsing utility.\n",
    "\n",
    "# dictionary needed for evaluation\n",
    "\n",
    "def getLinksEval(wikicode):\n",
    "    link_dict={}\n",
    "    linklist = wtp.parse(str(wikicode)).wikilinks\n",
    "    for l in linklist:\n",
    "        link,anchor = wtpGetLinkAnchor(l)\n",
    "        link = redirects.get(link,link)\n",
    "        link_dict[anchor] = link\n",
    "#         link_dict[l.plain_text().strip()] = l.title.strip()\n",
    "    return link_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:31:58.369531Z",
     "start_time": "2020-09-10T03:31:58.358643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split a MWPFH node <TEXT> into sentences\n",
    "SENT_ENDS = [u\".\", u\"!\", u\"?\"]\n",
    "def tokenize_sentence_split(text):\n",
    "    for line in text.split(\"\\n\"):\n",
    "        tok_acc = []\n",
    "        for tok in nltk.word_tokenize(line):\n",
    "            tok_acc.append(tok)\n",
    "            if tok in SENT_ENDS:\n",
    "                yield \" \".join(tok_acc)\n",
    "                tok_acc = []\n",
    "        if tok_acc:\n",
    "            yield \" \".join(tok_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:32:09.712283Z",
     "start_time": "2020-09-10T03:32:09.692385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Actual Linking function\n",
    "def process_page(page, page_wikicode):\n",
    "    page_wikicode_init= str(page_wikicode) # save the initial state\n",
    "    linked_mentions, linked_links = set(), set()\n",
    "    tested_mentions = set()\n",
    "    linked_mentions, linked_links = getLinks(page_wikicode, page) \n",
    "    ## this will only add the page-title as already linked\n",
    "\n",
    "    for gram_length in range(10, 0, -1):\n",
    "        #print(\"Scanning \", gram_length, \"Grams\")\n",
    "        # Parsing the tree can be done once\n",
    "        for node in page_wikicode.filter(recursive= False):\n",
    "            if isinstance(node, Text):\n",
    "                lines = node.split(\"\\n\")\n",
    "                for line in lines:\n",
    "                    for sent in tokenize_sentence_split(line):\n",
    "                        grams = list(ngrams(sent.split(), gram_length))\n",
    "                        for gram in grams:\n",
    "                            mention = ' '.join(gram).lower()## anchor dict is lower case\n",
    "                            mention_original = ' '.join(gram)##insert the un-lowercase mention\n",
    "                            # if the mention exist in the DB \n",
    "                            # it was not previously linked (or part of a link)\n",
    "                            # none of its candidate links is already used\n",
    "                            # it was not tested before (for efficiency)\n",
    "                            if (mention in anchors and\n",
    "                                not any(mention in s for s in linked_mentions) and\n",
    "                                not bool(set(anchors[mention].keys()) & linked_links) and\n",
    "                                mention not in tested_mentions):\n",
    "                                #logic\n",
    "                                #print(\"testing:\", mention, len(enanchors[mention]))\n",
    "                                candidate = classify_links(page, mention, THRESHOLD)\n",
    "                                if candidate:\n",
    "                                    candidate_link, candidate_proba = candidate\n",
    "                                    #print(\">> \", mention, candidate)\n",
    "                                    ############## Critical ##############\n",
    "                                    # Insert The Link in the current wikitext\n",
    "                                    match = re.compile(r'(?<!\\[\\[)(?<!-->)\\b{}\\b(?![\\w\\s]*[\\]\\]])'.format(re.escape(mention_original)))\n",
    "                                    newval, found = match.subn(\"[[\" + candidate_link  +  \"|\" + mention_original +\"]]\", node.value, 1)\n",
    "                                    node.value = newval\n",
    "                                    ######################################\n",
    "                                    # Book-keeping\n",
    "                                    linked_mentions.add(mention)\n",
    "                                    linked_links.add(candidate)\n",
    "                                # More Book-keeping\n",
    "                                tested_mentions.add(mention)\n",
    "\n",
    "    return page_wikicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:32:15.987721Z",
     "start_time": "2020-09-10T03:32:15.965247Z"
    }
   },
   "outputs": [],
   "source": [
    "# enanchors['unincorporated community']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:32:21.865462Z",
     "start_time": "2020-09-10T03:32:21.841755Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/home/mgerlach/REPOS/mwaddlink/scripts/utils_features.py:15: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  dst = (np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "micro_precision:\t 0.4227941176470588\n",
      "micro_recall:\t 0.5088495575221239\n",
      "----------------------\n",
      "micro_precision:\t 0.4520547945205479\n",
      "micro_recall:\t 0.5384615384615384\n",
      "----------------------\n",
      "micro_precision:\t 0.4371794871794872\n",
      "micro_recall:\t 0.5456\n",
      "----------------------\n",
      "micro_precision:\t 0.43243243243243246\n",
      "micro_recall:\t 0.5370370370370371\n",
      "----------------------\n",
      "micro_precision:\t 0.45045045045045046\n",
      "micro_recall:\t 0.5519779208831647\n",
      "----------------------\n",
      "micro_precision:\t 0.44594594594594594\n",
      "micro_recall:\t 0.5487528344671202\n",
      "----------------------\n",
      "micro_precision:\t 0.45959051724137934\n",
      "micro_recall:\t 0.5589777195281782\n",
      "----------------------\n",
      "micro_precision:\t 0.44636502287747837\n",
      "micro_recall:\t 0.5363469761759316\n",
      "----------------------\n",
      "micro_precision:\t 0.4492619926199262\n",
      "micro_recall:\t 0.5331143951833607\n",
      "----------------------\n",
      "micro_precision:\t 0.4450413223140496\n",
      "micro_recall:\t 0.527165932452276\n",
      "micro_precision:\t 0.4450413223140496\n",
      "micro_recall:\t 0.527165932452276\n",
      "CPU times: user 2h 15min 32s, sys: 41.3 s, total: 2h 16min 13s\n",
      "Wall time: 9min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Running the Model on a page\n",
    "\n",
    "# Process pages of interest\n",
    "# page_names = [\"De_Lassone\", \"13463_Antiphos\", \"Peter_Jungen\", \"AVT\"]\n",
    "\n",
    "THRESHOLD = 0.9\n",
    "\n",
    "### eval vars (micro and macro)\n",
    "count_doc = 0.\n",
    "count_docp = 0.\n",
    "macro_pre = 0.\n",
    "macro_rec = 0.\n",
    "tot_TP = 0.\n",
    "tot_rel = 0.\n",
    "tot_ret = 0.\n",
    "\n",
    "#### Backtest\n",
    "for page, page_wikicode in test_set[:1000]:\n",
    "#     if page != 'Taylor Negron':\n",
    "#         continue\n",
    "    input_code = page_wikicode\n",
    "    output_code = process_page(page, mwph.parse(mwph.parse(page_wikicode).strip_code()))\n",
    "    inp_pairs = getLinksEval(input_code)\n",
    "    out_pairs = getLinksEval(output_code)\n",
    "\n",
    "    TP = dict(set(inp_pairs.items()).intersection(out_pairs.items()))\n",
    "    #\n",
    "    doc_pre = 0 if len(out_pairs)==0 else len(TP)/len(out_pairs)\n",
    "    doc_rec = len(TP)/len(inp_pairs)\n",
    "    #\n",
    "    tot_TP  += len(TP)\n",
    "    tot_ret += len(out_pairs)\n",
    "    tot_rel += len(inp_pairs)\n",
    "    #print(len(TP), len(inp_pairs), len(out_pairs), \" P:\", doc_pre, \" R:\", doc_rec)\n",
    "    count_doc+=1\n",
    "    if count_doc %100 == 0:\n",
    "        print('----------------------')\n",
    "        micro_precision = tot_TP/tot_ret\n",
    "        micro_recall    = tot_TP/tot_rel\n",
    "        print(\"micro_precision:\\t\", micro_precision)\n",
    "        print(\"micro_recall:\\t\"   , micro_recall)\n",
    "\n",
    "micro_precision = tot_TP/tot_ret\n",
    "micro_recall    = tot_TP/tot_rel\n",
    "print(\"micro_precision:\\t\",micro_precision)\n",
    "print(\"micro_recall:\\t\",  micro_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brad Stephen \"Taylor\" Negron (August 1, 1957 – January 10, 2015) was an [[Americans|American]] [[writer]], [[actor]], [[painter]], and [[stand-up comedian]].\\n'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brad Stephen \"Taylor\" Negron ([[August 1|August 1]], 1957 – [[January 10|January 10]], 2015) was an American [[Writer|writer]], [[Actor|actor]], [[Painting|painter]], and [[Stand-up comedy|stand-up comedian]].'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Americans', 0.42265084)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_links('Taylor Negron','american',0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Painting'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redirects['Painter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'american': 'Americans',\n",
       " 'writer': 'Writer',\n",
       " 'actor': 'Actor',\n",
       " 'painter': 'Painting',\n",
       " 'stand-up comedian': 'Stand-up comedy'}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'august 1': 'August 1',\n",
       " 'january 10': 'January 10',\n",
       " 'writer': 'Writer',\n",
       " 'actor': 'Actor',\n",
       " 'painter': 'Painting',\n",
       " 'stand-up comedian': 'Stand-up comedy'}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brad Stephen \"Taylor\" Negron (August 1, 1957 – January 10, 2015) was an [[Americans|American]] [[writer]], [[actor]], [[painter]], and [[stand-up comedian]].\\n'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brad Stephen \"Taylor\" Negron (August 1, 1957 – January 10, 2015) was an American writer, actor, painter, and stand-up comedian.'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mwph.parse(mwph.parse(page_wikicode).strip_code())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mwaddlink",
   "language": "python",
   "name": "venv_mwaddlink"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
