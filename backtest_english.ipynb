{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:30:08.669999Z",
     "start_time": "2020-09-10T03:30:08.657997Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import shelve\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import mwparserfromhell as mwph\n",
    "from mwparserfromhell.nodes.text import Text\n",
    "from mwparserfromhell.nodes.wikilink import Wikilink \n",
    "import wikitextparser as wtp\n",
    "\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import operator\n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "import time\n",
    "import operator\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:30:17.261278Z",
     "start_time": "2020-09-10T03:30:17.089482Z"
    }
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "need 'c' or 'n' flag to open new db",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7f305d3366da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# this is generated by script: ./scripts/generate_anchor_dictionary.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# enanchors = pickle.load( open( \"./data/en/en.anchors.pkl\", \"rb\" ) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0menanchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/en/en.anchors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/shelve.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, flag, protocol, writeback)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \"\"\"\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDbfilenameShelf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/shelve.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, flag, protocol, writeback)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mShelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/dbm/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(file, flag, mode)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_defaultmod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"need 'c' or 'n' flag to open new db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# db type cannot be determined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: need 'c' or 'n' flag to open new db"
     ]
    }
   ],
   "source": [
    "# Load the anchor dictionary (the main data structure)\n",
    "# this is generated by script: ./scripts/generate_anchor_dictionary.py\n",
    "# enanchors = pickle.load( open( \"./data/en/en.anchors.pkl\", \"rb\" ) )\n",
    "enanchors = shelve.open('./data/en/en.anchors', flag='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:30:27.193197Z",
     "start_time": "2020-09-10T03:30:27.189187Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: SVD as additional component\n",
    "# We want for all pages .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:30:32.061048Z",
     "start_time": "2020-09-10T03:30:32.037436Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/en/training/sentences_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dffd23eb9008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the sentences to test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/en/training/sentences_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/en/training/sentences_test.csv'"
     ]
    }
   ],
   "source": [
    "# Load the sentences to test\n",
    "test_set = []\n",
    "with open('./data/en/training/sentences_test.csv') as fin:\n",
    "    for line in fin:\n",
    "        try:\n",
    "            title, sent = line.split('\\t')\n",
    "            test_set.append((title, sent))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:30:39.791455Z",
     "start_time": "2020-09-10T03:30:39.568034Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/en/en.w2v.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8f7adca26dc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwikipedia2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWikipedia2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mw2file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/en/en.w2v.bin'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWikipedia2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/pyt37/lib/python3.7/site-packages/wikipedia2vec/wikipedia2vec.pyx\u001b[0m in \u001b[0;36mwikipedia2vec.wikipedia2vec.Wikipedia2Vec.load\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/pyt37/lib/python3.7/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/en/en.w2v.bin'"
     ]
    }
   ],
   "source": [
    "# Embeddings of Wikipedia entities(not words)\n",
    "# this is generated by script: wikipedia2vec train --min-entity-count=0 --dim-size 100 enwiki-latest-pages-articles.xml.bz2 ./data/en/en.w2v.bin\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "w2file = './data/en/en.w2v.bin'\n",
    "word2vec = Wikipedia2Vec.load(w2file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:30:45.061002Z",
     "start_time": "2020-09-10T03:30:45.033204Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "./data/en/word2vec_enwiki_params-cbow-50-5-0.1-10-5-20.bin cannot be opened for loading!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9d548bd64292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnavfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/en/word2vec_enwiki_params-cbow-50-5-0.1-10-5-20.bin'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnav2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnavfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/pyt37/lib/python3.7/site-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;34m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0meprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyt37/lib/python3.7/site-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, args)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasttext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ./data/en/word2vec_enwiki_params-cbow-50-5-0.1-10-5-20.bin cannot be opened for loading!"
     ]
    }
   ],
   "source": [
    "# Navigation embeddings\n",
    "# TODO: page_tile as key @Martin\n",
    "# Ideal: have a vector for ALL the wikipedia pages\n",
    "\n",
    "# TODO: Check if we can load this with mmap\n",
    "\n",
    "import fasttext\n",
    "navfile = './data/en/word2vec_enwiki_params-cbow-50-5-0.1-10-5-20.bin'\n",
    "nav2vec = fasttext.load_model(navfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:30:50.475678Z",
     "start_time": "2020-09-10T03:30:50.455586Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/en/pageid.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9c4400832d00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield_size_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/en/pageid.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpageid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/en/pageid.csv'"
     ]
    }
   ],
   "source": [
    "# TODO: the navigation model should change to (page_title, vector)\n",
    "# This piece won't be needed then\n",
    "\n",
    "# TODO: It is probably easier to extract the page_id from the dump\n",
    "# and streamline this step\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "reader = csv.reader(open('./data/en/pageid.csv', 'r'))\n",
    "pageid = {}\n",
    "for row in reader:\n",
    "    k, v = row[0].split('\\t')\n",
    "    pageid[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:30:56.960702Z",
     "start_time": "2020-09-10T03:30:56.942432Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-315418c73599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# List of word embedded 'entities'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mveclist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'word2vec' is not defined"
     ]
    }
   ],
   "source": [
    "# List of word embedded 'entities'\n",
    "veclist = set([t.title for t in list(word2vec.dictionary.entities())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:31:03.710581Z",
     "start_time": "2020-09-10T03:31:02.908897Z"
    }
   },
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[05:31:03] /workspace/dmlc-core/src/io/local_filesys.cc:209: Check failed: allow_null:  LocalFileSystem::Open \"./data/en/0001.link.bin\": No such file or directory\nStack trace:\n  [bt] (0) /home/ubuntu/pyt37/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(dmlc::io::LocalFileSystem::Open(dmlc::io::URI const&, char const*, bool)+0x506) [0x7f25b53735e6]\n  [bt] (1) /home/ubuntu/pyt37/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(dmlc::Stream::Create(char const*, char const*, bool)+0x39) [0x7f25b534ab59]\n  [bt] (2) /home/ubuntu/pyt37/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterLoadModel+0x8a3) [0x7f25b4fac1a3]\n  [bt] (3) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f2741d62e40]\n  [bt] (4) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x2eb) [0x7f2741d628ab]\n  [bt] (5) /home/ubuntu/pyt37/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2be) [0x7f2741f767fe]\n  [bt] (6) /home/ubuntu/pyt37/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x10ab4) [0x7f2741f75ab4]\n  [bt] (7) /home/ubuntu/pyt37/bin/python3.7(_PyObject_FastCallKeywords+0x8a) [0x4e81fa]\n  [bt] (8) /home/ubuntu/pyt37/bin/python3.7(_PyEval_EvalFrameDefault+0x51df) [0x55e8ef]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f0630f5802be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# init model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/en/0001.link.bin'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# make a random test of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyt37/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_Booster'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Booster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'n_jobs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Booster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Booster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scikit_learn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyt37/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m   1802\u001b[0m             \u001b[0;31m# from URL.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m             _check_call(_LIB.XGBoosterLoadModel(\n\u001b[0;32m-> 1804\u001b[0;31m                 self.handle, c_str(os_fspath(fname))))\n\u001b[0m\u001b[1;32m   1805\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyt37/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \"\"\"\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [05:31:03] /workspace/dmlc-core/src/io/local_filesys.cc:209: Check failed: allow_null:  LocalFileSystem::Open \"./data/en/0001.link.bin\": No such file or directory\nStack trace:\n  [bt] (0) /home/ubuntu/pyt37/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(dmlc::io::LocalFileSystem::Open(dmlc::io::URI const&, char const*, bool)+0x506) [0x7f25b53735e6]\n  [bt] (1) /home/ubuntu/pyt37/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(dmlc::Stream::Create(char const*, char const*, bool)+0x39) [0x7f25b534ab59]\n  [bt] (2) /home/ubuntu/pyt37/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterLoadModel+0x8a3) [0x7f25b4fac1a3]\n  [bt] (3) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f2741d62e40]\n  [bt] (4) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x2eb) [0x7f2741d628ab]\n  [bt] (5) /home/ubuntu/pyt37/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2be) [0x7f2741f767fe]\n  [bt] (6) /home/ubuntu/pyt37/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x10ab4) [0x7f2741f75ab4]\n  [bt] (7) /home/ubuntu/pyt37/bin/python3.7(_PyObject_FastCallKeywords+0x8a) [0x4e81fa]\n  [bt] (8) /home/ubuntu/pyt37/bin/python3.7(_PyEval_EvalFrameDefault+0x51df) [0x55e8ef]\n\n"
     ]
    }
   ],
   "source": [
    "# Load the moodel classifier\n",
    "\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier()  # init model\n",
    "model.load_model('./data/en/0001.link.bin')  # load data\n",
    "\n",
    "# make a random test of the model\n",
    "# model.predict_proba(np.array([2, 36567, 669, 726.889369, 0.558646, 0.0]).reshape((1,-1)))[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:31:13.168613Z",
     "start_time": "2020-09-10T03:31:13.157304Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to factor out\n",
    "\n",
    "# Wikipedia2Vec distance between two pages (origin, destination)\n",
    "# Semantic relationship\n",
    "def getW2VDst(ent_a, ent_b):\n",
    "    dst = 0\n",
    "    if ent_a in veclist and ent_b in veclist:\n",
    "        a = word2vec.get_entity_vector(ent_a)\n",
    "        b = word2vec.get_entity_vector(ent_b)\n",
    "        dst = (np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b))\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:31:18.436071Z",
     "start_time": "2020-09-10T03:31:18.423629Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to factor out\n",
    "\n",
    "# Navigation distance between two pages (origin, destination)\n",
    "# Probability of navigation (see with Martin)\n",
    "def getNavDst(ent_a, ent_b):\n",
    "    dst = 0\n",
    "    if ent_a in pageid and ent_b in pageid:\n",
    "        page_a = pageid[ent_a]\n",
    "        page_b = pageid[ent_b]\n",
    "        if ent_a in veclist and ent_b in veclist:\n",
    "            a = nav2vec.get_word_vector(page_a)\n",
    "            b = nav2vec.get_word_vector(page_b)\n",
    "            dst = (np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b))\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:31:23.451651Z",
     "start_time": "2020-09-10T03:31:23.447565Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Pre-processing\n",
    "\n",
    "# # TODO: The following block should be replaced by finding the list of valid pages\n",
    "# # to link from the Hive database\n",
    "\n",
    "\n",
    "########################\n",
    "# # Description: Collect a list of entities, disambiguation pages, redirects\n",
    "# # to exclude them as candidates\n",
    "\n",
    "# # The following file is from the dumps\n",
    "# entities = []\n",
    "# with open('./data/en_bis/enwiki-20200201-all-titles-in-ns0') as fin:\n",
    "#     for line in fin:\n",
    "#         entities.append(line.strip().replace(\"_\", \" \"))\n",
    "\n",
    "# # The following file is found in the redirects (I think it's available in the dumps too)\n",
    "# dislinks = []\n",
    "# with open('./data/en_bis/enwiki.dis') as fin:\n",
    "#     for line in fin:\n",
    "#         dislinks.append(line.strip())\n",
    "\n",
    "# # Finally Remove the disambiguation links\n",
    "# dislinks = set(dislinks)\n",
    "# entities = set(entities)  - dislinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:31:28.801481Z",
     "start_time": "2020-09-10T03:31:28.783129Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'veclist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9e9485a76c76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Num of potential target pages:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mveclist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'veclist' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Num of potential target pages:\", len(veclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:31:34.855430Z",
     "start_time": "2020-09-10T03:31:34.842086Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to factor out\n",
    "\n",
    "# Return the features for each link candidate in the context of the text and the page\n",
    "# TODO: refactor this piece of code to be the same for training model\n",
    "def get_feature_set(page, text, link):\n",
    "    ngram = len(text.split()) # simple space based tokenizer to compute n-grams\n",
    "    freq = enanchors[text][link] # How many times was the link use with this text \n",
    "    ambig = len(enanchors[text]) # home many different links where used with this text\n",
    "    kur = kurtosis(sorted(list(enanchors[text].values()), reverse = True) + [1] * (1000 - ambig)) # Skew of usage text/link distribution\n",
    "    w2v = getW2VDst(page, link) # W2V Distance between the source and target page\n",
    "    nav = getNavDst(page, link) # Nav Distance between the source and target page\n",
    "    return (ngram, freq, ambig, kur, w2v, nav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:31:40.123839Z",
     "start_time": "2020-09-10T03:31:40.109073Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main decision function.\n",
    "\n",
    "# for a given page X and a piece of text \"lipsum\".. check all the candidate and make inference\n",
    "# Returns the most likely candidate according to the pre-trained link model\n",
    "# If the probability is below a certain threshold, return None\n",
    "def classify_links(page, text, THRESHOLD):\n",
    "    #start_time = time.time()\n",
    "    cand_prediction = {}\n",
    "    # Work with the 10 most frequent candidates\n",
    "    limited_cands = enanchors[text]\n",
    "    if len(limited_cands) > 10:\n",
    "        limited_cands = dict(sorted(enanchors[text].items(), key = operator.itemgetter(1), reverse = True)[:10]) \n",
    "    for cand in limited_cands:\n",
    "        # get the features\n",
    "        cand_feats = get_feature_set(page, text, cand)\n",
    "        # compute the model probability\n",
    "        cand_prediction[cand] = model.predict_proba(np.array(cand_feats).reshape((1,-1)))[0,1]\n",
    "    \n",
    "    # Compute the top candidate\n",
    "    top_candidate = max(cand_prediction.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    # Check if the max probability meets the threshold before returning\n",
    "    if top_candidate[1] < THRESHOLD:\n",
    "        return None\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return top_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:31:47.643912Z",
     "start_time": "2020-09-10T03:31:47.631050Z"
    }
   },
   "outputs": [],
   "source": [
    "# Article parsing utility.\n",
    "\n",
    "# For a given page return the list of all existing links and mentions\n",
    "# To avoid linking what's already linked\n",
    "def getLinks(wikicode, page_title):\n",
    "    m = set()\n",
    "    e = set()\n",
    "    if page_title:\n",
    "        page_title_tmp = page_title.replace('_',' ')\n",
    "        # add the page title itself\n",
    "        m.add(page_title_tmp)\n",
    "        e.add(page_title_tmp)\n",
    "    linklist = wtp.parse(str(wikicode)).wikilinks\n",
    "    for l in linklist:\n",
    "        m.add(l.plain_text().strip())\n",
    "        e.add(l.title.strip())\n",
    "    return m, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:31:52.975212Z",
     "start_time": "2020-09-10T03:31:52.966609Z"
    }
   },
   "outputs": [],
   "source": [
    "# Article parsing utility.\n",
    "\n",
    "# dictionary needed for evaluation\n",
    "\n",
    "def getLinksEval(wikicode):\n",
    "    link_dict={}\n",
    "    linklist = wtp.parse(str(wikicode)).wikilinks\n",
    "    for l in linklist:\n",
    "        link_dict[l.plain_text().strip()] = l.title.strip()\n",
    "    return link_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:31:58.369531Z",
     "start_time": "2020-09-10T03:31:58.358643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Article parsing utility.\n",
    "\n",
    "# Split a MWPFH node <TEXT> into sentences\n",
    "SENT_ENDS = [u\".\", u\"!\", u\"?\"]\n",
    "def tokenize_sentence_split(text):\n",
    "    for line in text.split(\"\\n\"):\n",
    "        tok_acc = []\n",
    "        for tok in nltk.word_tokenize(line):\n",
    "            tok_acc.append(tok)\n",
    "            if tok in SENT_ENDS:\n",
    "                yield \" \".join(tok_acc)\n",
    "                tok_acc = []\n",
    "        if tok_acc:\n",
    "            yield \" \".join(tok_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:32:05.002864Z",
     "start_time": "2020-09-10T03:32:04.991825Z"
    }
   },
   "outputs": [],
   "source": [
    "# Article parsing utility.\n",
    "\n",
    "# This is in an effort to correct the redirects in the lists\n",
    "def postProcessList(list_ent):\n",
    "    correct_dict = {}\n",
    "    for k in list_ent:\n",
    "        if word2vec.get_entity(k):\n",
    "            redirect = word2vec.get_entity(k).title\n",
    "            correct_dict[redirect] = correct_dict.get(redirect, 0) + list_ent[k]\n",
    "        else:\n",
    "            correct_dict[k] = correct_dict.get(k, 0) + list_ent[k]\n",
    "    return correct_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:32:09.712283Z",
     "start_time": "2020-09-10T03:32:09.692385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Actual Linking function\n",
    "def process_page(page, page_wikicode):\n",
    "    page_wikicode_init= str(page_wikicode) # save the initial state\n",
    "    linked_mentions, linked_links = set(), set()\n",
    "    tested_mentions = set()\n",
    "    for gram_length in range(10, 1, -1):\n",
    "        #print(\"Scanning \", gram_length, \"Grams\")\n",
    "        # Parsing the tree can be done once\n",
    "        for node in page_wikicode.filter(recursive= False):\n",
    "            if isinstance(node, Text):\n",
    "                lines = node.split(\"\\n\")\n",
    "                for line in lines:\n",
    "                    for sent in tokenize_sentence_split(line):\n",
    "                        grams = list(ngrams(sent.split(), gram_length))\n",
    "                        for gram in grams:\n",
    "                            mention = ' '.join(gram)\n",
    "                            # if the mention exist in the DB \n",
    "                            # it was not previously linked (or part of a link)\n",
    "                            # none of its candidate links is already used\n",
    "                            # it was not tested before (for efficiency)\n",
    "                            if (mention in enanchors and\n",
    "                                not any(mention in s for s in linked_mentions) and\n",
    "                                not bool(set(enanchors[mention].keys()) & linked_links) and\n",
    "                                mention not in tested_mentions):\n",
    "                                #logic\n",
    "                                #print(\"testing:\", mention, len(enanchors[mention]))\n",
    "                                candidate = classify_links(page, mention, THRESHOLD)\n",
    "                                if candidate:\n",
    "                                    candidate_link, candidate_proba = candidate\n",
    "                                    #print(\">> \", mention, candidate)\n",
    "                                    ############## Critical ##############\n",
    "                                    # Insert The Link in the current wikitext\n",
    "                                    match = re.compile(r'(?<!\\[\\[)(?<!-->)\\b{}\\b(?![\\w\\s]*[\\]\\]])'.format(re.escape(mention)))\n",
    "                                    newval, found = match.subn(\"[[\" + candidate_link  +  \"|\" + mention +\"]]\", node.value, 1)\n",
    "                                    node.value = newval\n",
    "                                    ######################################\n",
    "                                    # Book-keeping\n",
    "                                    linked_mentions.add(mention)\n",
    "                                    linked_links.add(candidate)\n",
    "                                # More Book-keeping\n",
    "                                tested_mentions.add(mention)\n",
    "\n",
    "    return page_wikicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:32:15.987721Z",
     "start_time": "2020-09-10T03:32:15.965247Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enanchors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6638e1a0a188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menanchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unincorporated community'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'enanchors' is not defined"
     ]
    }
   ],
   "source": [
    "enanchors['unincorporated community']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:32:21.865462Z",
     "start_time": "2020-09-10T03:32:21.841755Z"
    }
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Running the Model on a page\n",
    "\n",
    "# Process pages of interest\n",
    "# page_names = [\"De_Lassone\", \"13463_Antiphos\", \"Peter_Jungen\", \"AVT\"]\n",
    "\n",
    "THRESHOLD = 0.95\n",
    "\n",
    "### eval vars (micro and macro)\n",
    "count_doc = 0.\n",
    "count_docp = 0.\n",
    "macro_pre = 0.\n",
    "macro_rec = 0.\n",
    "tot_TP = 0.\n",
    "tot_rel = 0.\n",
    "tot_ret = 0.\n",
    "\n",
    "#### Backtest\n",
    "for page, page_wikicode in test_set:\n",
    "    input_code = page_wikicode\n",
    "    output_code = process_page(page, mwph.parse(mwph.parse(page_wikicode).strip_code()))\n",
    "    inp_pairs = getLinksEval(input_code)\n",
    "    out_pairs = getLinksEval(output_code)\n",
    "    #\n",
    "    TP = dict(set(inp_pairs.items()).intersection(out_pairs.items()))\n",
    "    #\n",
    "    doc_pre = 0 if len(out_pairs)==0 else len(TP)/len(out_pairs)\n",
    "    doc_rec = len(TP)/len(inp_pairs)\n",
    "    #\n",
    "    tot_TP  += len(TP)\n",
    "    tot_ret += len(out_pairs)\n",
    "    tot_rel += len(inp_pairs)\n",
    "    #print(len(TP), len(inp_pairs), len(out_pairs), \" P:\", doc_pre, \" R:\", doc_rec)\n",
    "    count_doc+=1\n",
    "    if count_doc %100 == 0:\n",
    "        print('----------------------')\n",
    "        micro_precision = tot_TP/tot_ret\n",
    "        micro_recall    = tot_TP/tot_rel\n",
    "        print(\"micro_precision:\\t\", micro_precision)\n",
    "        print(\"micro_recall:\\t\"   , micro_recall)\n",
    "\n",
    "micro_precision = tot_TP/tot_ret\n",
    "micro_recall    = tot_TP/tot_rel\n",
    "print(\"micro_precision:\\t\",micro_precision)\n",
    "print(\"micro_recall:\\t\",  micro_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt37",
   "language": "python",
   "name": "pyt37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
