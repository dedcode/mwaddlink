{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import shelve\n",
    "import mwparserfromhell\n",
    "from mwparserfromhell.nodes.text import Text\n",
    "from mwparserfromhell.nodes.wikilink import Wikilink \n",
    "import wikitextparser as wtp\n",
    "\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import operator\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from scripts.utils import wtpGetLinkAnchor\n",
    "from scripts.utils_features import get_feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(sys.argv) >= 2:\n",
    "#     lang = sys.argv[1]\n",
    "# else:\n",
    "#     lang = 'en'\n",
    "lang = 'simple'\n",
    "wiki   = lang+'wiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://{0}.wikipedia.org/w/api.php\".format(lang)\n",
    "\n",
    "def parse(title):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"rvlimit\": 1,\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\",\n",
    "        \"formatversion\": \"2\",\n",
    "    }\n",
    "    headers = {\"User-Agent\": \"My-Bot-Name/1.0\"}\n",
    "    req = requests.get(API_URL, headers=headers, params=params)\n",
    "    res = req.json()\n",
    "    revision = res[\"query\"][\"pages\"][0][\"revisions\"][0]\n",
    "    text = revision[\"slots\"][\"main\"][\"content\"]\n",
    "    return mwparserfromhell.parse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## open datasets as shelve\n",
    "# Load the anchor dictionary (the main data structure)\n",
    "anchors = shelve.open( \"./data/{0}/{0}.anchors.db\".format(lang), flag='r' )\n",
    "pageids = shelve.open( \"./data/{0}/{0}.pageids.db\".format(lang), flag='r' )\n",
    "redirects = shelve.open( \"./data/{0}/{0}.redirects.db\".format(lang), flag='r' )\n",
    "\n",
    "## load word2vec features\n",
    "word2vec = shelve.open(\"./data/{0}/{0}.w2v.filtered.db\".format(lang), flag='r' )\n",
    "## load navigation-vector features\n",
    "nav2vec = shelve.open(\"./data/{0}/{0}.nav.filtered.db\".format(lang), flag='r' )\n",
    "\n",
    "## load trained model\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier()  # init model\n",
    "model.load_model('./data/{0}/{0}.linkmodel.bin'.format(lang))  # load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main decision function.\n",
    "\n",
    "# for a given page X and a piece of text \"lipsum\".. check all the candidate and make inference\n",
    "# Returns the most likely candidate according to the pre-trained link model\n",
    "# If the probability is below a certain threshold, return None\n",
    "def classify_links(page, text, THRESHOLD):\n",
    "    #start_time = time.time()\n",
    "    cand_prediction = {}\n",
    "    # Work with the 10 most frequent candidates\n",
    "    limited_cands = anchors[text]\n",
    "    if len(limited_cands) > 10:\n",
    "        limited_cands = dict(sorted(anchors[text].items(), key = operator.itemgetter(1), reverse = True)[:10]) \n",
    "    for cand in limited_cands:\n",
    "        # get the features\n",
    "#         cand_feats = get_feature_set(page, text, cand, anchors, word2vec,nav2vec,pageids)\n",
    "        cand_feats = get_feature_set(page, text, cand, anchors, word2vec,nav2vec)\n",
    "\n",
    "        # compute the model probability\n",
    "        cand_prediction[cand] = model.predict_proba(np.array(cand_feats).reshape((1,-1)))[0,1]\n",
    "    \n",
    "    # Compute the top candidate\n",
    "    top_candidate = max(cand_prediction.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    # Check if the max probability meets the threshold before returning\n",
    "    if top_candidate[1] < THRESHOLD:\n",
    "        return None\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return top_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article parsing utility.\n",
    "\n",
    "# For a given page return the list of all existing links and mentions\n",
    "# To avoid linking what's already linked\n",
    "def getLinks(wikicode, page_title):\n",
    "    m = set()\n",
    "    e = set()\n",
    "    page_title_tmp = page_title.replace('_',' ')\n",
    "    # add the page title itself\n",
    "    m.add(page_title_tmp)\n",
    "    e.add(page_title_tmp)\n",
    "    linklist = wtp.parse(str(wikicode)).wikilinks\n",
    "    for l in linklist:\n",
    "        link,anchor = wtpGetLinkAnchor(l)\n",
    "        m.add(anchor)\n",
    "        e.add(link)\n",
    "#         m.add(l.plain_text().strip())\n",
    "#         e.add(l.title.strip())\n",
    "    return m, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article parsing utility.\n",
    "\n",
    "# Split a MWPFH node <TEXT> into sentences\n",
    "SENT_ENDS = [u\".\", u\"!\", u\"?\"]\n",
    "def tokenize_sentence_split(text):\n",
    "    for line in text.split(\"\\n\"):\n",
    "        tok_acc = []\n",
    "        for tok in nltk.word_tokenize(line):\n",
    "            tok_acc.append(tok)\n",
    "            if tok in SENT_ENDS:\n",
    "                yield \" \".join(tok_acc)\n",
    "                tok_acc = []\n",
    "        if tok_acc:\n",
    "            yield \" \".join(tok_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Linking function\n",
    "def process_page(page):\n",
    "    page_wikicode = parse(page)\n",
    "    page_wikicode_init= str(page_wikicode) # save the initial state\n",
    "    linked_mentions, linked_links = getLinks(page_wikicode, page)\n",
    "    tested_mentions = set()\n",
    "    for gram_length in range(10, 0, -1):\n",
    "        #print(\"Scanning \", gram_length, \"Grams\")\n",
    "        # Parsing the tree can be done once\n",
    "        for node in page_wikicode.filter(recursive= False):\n",
    "            if isinstance(node, Text):\n",
    "                lines = node.split(\"\\n\")\n",
    "                for line in lines:\n",
    "\n",
    "                    for sent in tokenize_sentence_split(line):\n",
    "                        grams = list(ngrams(sent.split(), gram_length))\n",
    "    \n",
    "                        for gram in grams:\n",
    "                            mention = ' '.join(gram).lower()\n",
    "                            # if the mention exist in the DB \n",
    "                            # it was not previously linked (or part of a link)\n",
    "                            # none of its candidate links is already used\n",
    "                            # it was not tested before (for efficiency)\n",
    " \n",
    "                            if (mention in anchors and\n",
    "                                not any(mention in s for s in linked_mentions) and\n",
    "                                not bool(set(anchors[mention].keys()) & linked_links) and\n",
    "                                mention not in tested_mentions):\n",
    "                                #logic\n",
    "                                #print(\"testing:\", mention, len(anchors[mention]))\n",
    "                                candidate = classify_links(page, mention, THRESHOLD)\n",
    "                                if candidate:\n",
    "                                    candidate_link, candidate_proba = candidate\n",
    "                                    #print(\">> \", mention, candidate)\n",
    "                                    ############## Critical ##############\n",
    "                                    # Insert The Link in the current wikitext\n",
    "                                    match = re.compile(r'(?<!\\[\\[)(?<!-->)\\b{}\\b(?![\\w\\s]*[\\]\\]])'.format(re.escape(mention)))\n",
    "                                    newval, found = match.subn(\"[[\" + candidate_link  +  \"|\" + mention+  \"|pr=\" + str(candidate_proba) + \"]]\", node.value, 1)\n",
    "                                    node.value = newval\n",
    "                                    ######################################\n",
    "                                    # Book-keeping\n",
    "                                    linked_mentions.add(mention)\n",
    "                                    linked_links.add(candidate)\n",
    "                                # More Book-keeping\n",
    "                                tested_mentions.add(mention)\n",
    "\n",
    "    return page_wikicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: Shri Yantra\n",
      "\n",
      "==========\n",
      "\n",
      "2.6180663108825684\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "# Running the Model on a page\n",
    "THRESHOLD = 0.8\n",
    "\n",
    "page_title = \"Fernand_LÃ©ger\"\n",
    "# page_title = \"Tarek_Kamel\"\n",
    "page_title = \"Shri Yantra\"\n",
    "\n",
    "print(\"processing:\", page_title)\n",
    "print(\"\\n==========\\n\")\n",
    "t1 = time.time()\n",
    "result = process_page(page_title)\n",
    "# print(result)\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{complex|date=June 2012}}\n",
      "[[Image:SriYantra color.svg|thumb|The Shri Yantra.]]\n",
      "The '''Shri Yantra''' or '''Sri Chakra''' of [[Tripura Sundari]] is a [[yantra]] or [[mandala]] formed by nine interlocking triangles surrounding a dot in the cetner called a [[bindu]].  Four of these triangles are orientated upright representing [[Shiva]] or the Masculine.  Five of these triangles are inverted triangles represent [[Shakti]] or the Feminine. Because it is composed of nine triangles, it is also known as the ''navayoni chakra''.<ref name=SC>{{cite book|last=Shankaranarayanan|first=S.|title=Sri Chakra|edition=3rd|year=1979|publisher=Dipti Publications}}</ref>\n",
      "\n",
      "Together the nine triangles are interlaced in such a way as to form [[43 (number)|43]] smaller triangles in a web symbolic of the entire [[Universe|cosmos|pr=0.8924864]] or a [[Uterus|womb|pr=0.99999416]] symbolic of creation. Together they express [[Advaita|Advaita-ism]] or [[non-duality]]. This is surrounded by a lotus of eight petals, a lotus of sixteen petals, and an earthsquare resembling a [[Temple|temple|pr=0.9678946]] with four doors.<ref name=SC/>\n",
      "\n",
      "The Shri Chakra is also known as the ''nava chakra'' because it can also be seen as having nine levels. Each level corresponds to a [[mudra]], a [[yogini]], and a specific form of the [[Deity|deity|pr=0.99997807]] [[Tripura Sundari]] along with her [[mantra]]. These levels starting from the outside or bottom layer are:<ref name=SC/>\n",
      "#''Trailokya Mohana'', a square of three lines with four portals\n",
      "#''Sarvasa Paripuraka'', a sixteen-petal lotus\n",
      "#''Sarva Sankshobahana'', an eight-petal lotus\n",
      "#''Sarva Saubhagyadayaka'', composed of fourteen small triangles\n",
      "#''Sarvarthasadhaka'', composed of ten small triangles\n",
      "#''Sarva Rakshakara'', composed of ten small triangles\n",
      "#''Sarva Rohahara'', composed of eight small triangles\n",
      "#''Sarva siddhi prada'', composed of 1 small [[Triangle|triangle|pr=0.9303133]]\n",
      "#''Sarvanandamaya'', composed of a point or ''bindu''\n",
      "\n",
      "==References==\n",
      "{{reflist}}\n",
      "\n",
      "==Other websites==\n",
      "\n",
      "\n",
      "*[http://www.shivashakti.com/tripura.htm Lalita Tripurasundari, the Red Goddess]\n",
      "*[https://narmadeshwarshrilaxmiyantramandir.com Shri Narmadeshwar Shri Laxmi Yantra Mandir]\n",
      "*[http://www.astrologyplus.in/shriyantra.htm Sri Yantra]\n",
      "*\n",
      "*[http://www.indoindians.com/festival/srichakra.htm How to Draw a Sri Chakra]\n",
      "*[http://www.bizzarocreations.com/gnosis/SriYantra.html How to draw Sri Yantra]\n",
      "\n",
      "[[Category:Hinduism]]\n",
      "[[Category:Meditation]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the crucial thing is to get the correct mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mwaddlink",
   "language": "python",
   "name": "venv_mwaddlink"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
