{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import mwparserfromhell\n",
    "from mwparserfromhell.nodes.text import Text\n",
    "from mwparserfromhell.nodes.wikilink import Wikilink \n",
    "import wikitextparser as wtp\n",
    "\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import operator\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from scripts.utils import wtpGetLinkAnchor\n",
    "from scripts.utils import get_feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(sys.argv) >= 2:\n",
    "#     lang = sys.argv[1]\n",
    "# else:\n",
    "#     lang = 'en'\n",
    "lang = 'simple'\n",
    "wiki   = lang+'wiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://{0}.wikipedia.org/w/api.php\".format(lang)\n",
    "\n",
    "def parse(title):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"rvlimit\": 1,\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\",\n",
    "        \"formatversion\": \"2\",\n",
    "    }\n",
    "    headers = {\"User-Agent\": \"My-Bot-Name/1.0\"}\n",
    "    req = requests.get(API_URL, headers=headers, params=params)\n",
    "    res = req.json()\n",
    "    revision = res[\"query\"][\"pages\"][0][\"revisions\"][0]\n",
    "    text = revision[\"slots\"][\"main\"][\"content\"]\n",
    "    return mwparserfromhell.parse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the anchor dictionary (the main data structure)\n",
    "# this is generated by script: ./scripts/generate_anchor_dictionary.py\n",
    "anchors = pickle.load( open( \"./data/{0}/{0}.anchors.pkl\".format(lang), \"rb\" ) )\n",
    "pageids = pickle.load( open( \"./data/{0}/{0}.pageids.pkl\".format(lang), \"rb\" ) )\n",
    "redirects = pickle.load( open( \"./data/{0}/{0}.redirects.pkl\".format(lang), \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## filter anchors\n",
    "# print(len(anchors))\n",
    "# anchors_tmp = {anchor:anchor_dict for anchor,anchor_dict in anchors.items() if sum(anchor_dict.values())>1}\n",
    "# anchors=anchors_tmp\n",
    "# print(len(anchors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: SVD as additional component\n",
    "# We want for all pages .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings of Wikipedia entities(not words)\n",
    "# this is generated by script: wikipedia2vec train --min-entity-count=0 --dim-size 100 enwiki-latest-pages-articles.xml.bz2 ./data/en/en.w2v.bin\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "w2file = './data/{0}/{0}.w2v.bin'.format(lang)\n",
    "word2vec = Wikipedia2Vec.load(w2file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigation embeddings\n",
    "# TODO: page_tile as key @Martin\n",
    "# Ideal: have a vector for ALL the wikipedia pages\n",
    "\n",
    "# TODO: Check if we can load this with mmap\n",
    "\n",
    "import fasttext\n",
    "navfile = './data/{0}/{0}.nav.bin'.format(lang)\n",
    "nav2vec = fasttext.load_model(navfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of word embedded 'entities'\n",
    "# veclist = set([t.title for t in list(word2vec.dictionary.entities())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the moodel classifier\n",
    "\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier()  # init model\n",
    "model.load_model('./data/{0}/0001.link.bin'.format(lang))  # load data\n",
    "\n",
    "# make a random test of the model\n",
    "# model.predict_proba(np.array([2, 36567, 669, 726.889369, 0.558646, 0.0]).reshape((1,-1)))[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main decision function.\n",
    "\n",
    "# for a given page X and a piece of text \"lipsum\".. check all the candidate and make inference\n",
    "# Returns the most likely candidate according to the pre-trained link model\n",
    "# If the probability is below a certain threshold, return None\n",
    "def classify_links(page, text, THRESHOLD):\n",
    "    #start_time = time.time()\n",
    "    cand_prediction = {}\n",
    "    # Work with the 10 most frequent candidates\n",
    "    limited_cands = anchors[text]\n",
    "    if len(limited_cands) > 10:\n",
    "        limited_cands = dict(sorted(anchors[text].items(), key = operator.itemgetter(1), reverse = True)[:10]) \n",
    "    for cand in limited_cands:\n",
    "        # get the features\n",
    "        cand_feats = get_feature_set(page, text, cand, anchors, word2vec,nav2vec,pageids)\n",
    "        # compute the model probability\n",
    "        cand_prediction[cand] = model.predict_proba(np.array(cand_feats).reshape((1,-1)))[0,1]\n",
    "    \n",
    "    # Compute the top candidate\n",
    "    top_candidate = max(cand_prediction.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    # Check if the max probability meets the threshold before returning\n",
    "    if top_candidate[1] < THRESHOLD:\n",
    "        return None\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return top_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article parsing utility.\n",
    "\n",
    "# For a given page return the list of all existing links and mentions\n",
    "# To avoid linking what's already linked\n",
    "def getLinks(wikicode, page_title):\n",
    "    m = set()\n",
    "    e = set()\n",
    "    page_title_tmp = page_title.replace('_',' ')\n",
    "    # add the page title itself\n",
    "    m.add(page_title_tmp)\n",
    "    e.add(page_title_tmp)\n",
    "    linklist = wtp.parse(str(wikicode)).wikilinks\n",
    "    for l in linklist:\n",
    "        link,anchor = wtpGetLinkAnchor(l)\n",
    "        m.add(anchor)\n",
    "        e.add(link)\n",
    "#         m.add(l.plain_text().strip())\n",
    "#         e.add(l.title.strip())\n",
    "    return m, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article parsing utility.\n",
    "\n",
    "# Split a MWPFH node <TEXT> into sentences\n",
    "SENT_ENDS = [u\".\", u\"!\", u\"?\"]\n",
    "def tokenize_sentence_split(text):\n",
    "    for line in text.split(\"\\n\"):\n",
    "        tok_acc = []\n",
    "        for tok in nltk.word_tokenize(line):\n",
    "            tok_acc.append(tok)\n",
    "            if tok in SENT_ENDS:\n",
    "                yield \" \".join(tok_acc)\n",
    "                tok_acc = []\n",
    "        if tok_acc:\n",
    "            yield \" \".join(tok_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article parsing utility.\n",
    "\n",
    "# # This is in an effort to correct the redirects in the lists\n",
    "# def postProcessList(list_ent):\n",
    "#     correct_dict = {}\n",
    "#     for k in list_ent:\n",
    "#         if word2vec.get_entity(k):\n",
    "#             redirect = word2vec.get_entity(k).title\n",
    "#             correct_dict[redirect] = correct_dict.get(redirect, 0) + list_ent[k]\n",
    "#         else:\n",
    "#             correct_dict[k] = correct_dict.get(k, 0) + list_ent[k]\n",
    "#     return correct_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample list of pages for testing\n",
    "# Process pages of interest\n",
    "# page_names = [\"De_Lassone\", \"13463_Antiphos\", \"Peter_Jungen\", \"AVT\"]\n",
    "\n",
    "page_names = [\"BMC_Amazon\",\n",
    "            \"Theory_of_regions\",\n",
    "            \"Pier_Luigi_Capucci\",\n",
    "            \"California_Digital_Library\",\n",
    "            \"Business_necessity\",\n",
    "            \"Bell_Post_Hill_Football_Club\",\n",
    "            \"Buying_center\",\n",
    "            \"Bay_(shelving)\",\n",
    "            \"National_Taichung_University_of_Education\",\n",
    "            \"Nalugu_Rallu_Aata\",\n",
    "            \"Nevis_Television\",\n",
    "            \"Tommy_Newberry\",\n",
    "            \"Nagraur,_Bahraich\",\n",
    "            \"Kris_Neely\",\n",
    "            \"National_Axe_Throwing_Federation\",\n",
    "            \"National_Arts_Council_of_South_Africa\",\n",
    "            \"Faisal_Tehrani\",\n",
    "            \"Thief_River_Falls_Times\",\n",
    "            \"Tambour_door\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Linking function\n",
    "def process_page(page):\n",
    "    page_wikicode = parse(page)\n",
    "    page_wikicode_init= str(page_wikicode) # save the initial state\n",
    "    linked_mentions, linked_links = getLinks(page_wikicode, page)\n",
    "    tested_mentions = set()\n",
    "    for gram_length in range(10, 0, -1):\n",
    "        #print(\"Scanning \", gram_length, \"Grams\")\n",
    "        # Parsing the tree can be done once\n",
    "        for node in page_wikicode.filter(recursive= False):\n",
    "            if isinstance(node, Text):\n",
    "                lines = node.split(\"\\n\")\n",
    "                for line in lines:\n",
    "\n",
    "                    for sent in tokenize_sentence_split(line):\n",
    "                        grams = list(ngrams(sent.split(), gram_length))\n",
    "    \n",
    "                        for gram in grams:\n",
    "                            mention = ' '.join(gram).lower()\n",
    "                            # if the mention exist in the DB \n",
    "                            # it was not previously linked (or part of a link)\n",
    "                            # none of its candidate links is already used\n",
    "                            # it was not tested before (for efficiency)\n",
    " \n",
    "                            if (mention in anchors and\n",
    "                                not any(mention in s for s in linked_mentions) and\n",
    "                                not bool(set(anchors[mention].keys()) & linked_links) and\n",
    "                                mention not in tested_mentions):\n",
    "                                #logic\n",
    "                                #print(\"testing:\", mention, len(anchors[mention]))\n",
    "                                candidate = classify_links(page, mention, THRESHOLD)\n",
    "                                if candidate:\n",
    "                                    candidate_link, candidate_proba = candidate\n",
    "                                    #print(\">> \", mention, candidate)\n",
    "                                    ############## Critical ##############\n",
    "                                    # Insert The Link in the current wikitext\n",
    "                                    match = re.compile(r'(?<!\\[\\[)(?<!-->)\\b{}\\b(?![\\w\\s]*[\\]\\]])'.format(re.escape(mention)))\n",
    "                                    newval, found = match.subn(\"[[\" + candidate_link  +  \"|\" + mention+  \"|pr=\" + str(candidate_proba) + \"]]\", node.value, 1)\n",
    "                                    node.value = newval\n",
    "                                    ######################################\n",
    "                                    # Book-keeping\n",
    "                                    linked_mentions.add(mention)\n",
    "                                    linked_links.add(candidate)\n",
    "                                # More Book-keeping\n",
    "                                tested_mentions.add(mention)\n",
    "\n",
    "    return page_wikicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: Fernand_Léger\n",
      "\n",
      "==========\n",
      "\n",
      "{{Infobox artist\n",
      "| name        = Fernand Léger\n",
      "| image       = Fernand Léger.jpg\n",
      "| imagesize   =\n",
      "| caption     = Fernand Léger photographed by {{nowrap|Carl Van Vechten, 1936}}\n",
      "| birth_name  = \n",
      "| birth_date  = {{birth date|1881|2|4|mf=y}}\n",
      "| birth_place = [[Argentan]], [[Orne]], [[French Third Republic|France]]\n",
      "| death_date  = {{death date and age|1955|8|17|1881|2|4|mf=y}}\n",
      "| death_place = [[Gif-sur-Yvette]], [[French Fourth Republic|France]]\n",
      "| nationality = [[French people|French]]\n",
      "| field       = [[Painting]], [[printmaking]] and [[filmmaking]]\n",
      "| training    = \n",
      "| movement    = [[Tubism]]<br>[[Cubism]]<br>[[Modernism]]}}\n",
      "\n",
      "'''Fernand Léger''' (Joseph Fernand Henri Léger, 4 February 1881 – 17 August 1955) was a French [[painting|painter]], [[sculpture|sculptor]], and [[film director|filmmaker]]. In his early works he created a personal form of [[cubism]] which he gradually [[Amendment|changed|pr=0.99994767]] into a [[More (song)|more|pr=0.99995637]] popular figurative style. His boldly simplified [[Therapy|treatment|pr=0.99524117]] of modern subject [[Matter|matter|pr=0.9999913]] made [[Hyun Soong-jong|him|pr=0.99994767]] a forerunner of [[pop art]].\n",
      "\n",
      "His personal style was called \"tubism\" [[Because (The Beatles song)|because|pr=0.99994767]] [[Historic counties of England|it was|pr=0.9999628]] [[Influence|influenced|pr=0.99993217]] by cubism, [[British United Traction|but|pr=0.99993217]] used tube-like forms.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{{DEFAULTSORT:Leger, Fernand}}\n",
      "[[Category:French painters]]\n",
      "[[Category:French sculptors]]\n",
      "[[Category:French filmmakers]]\n",
      "[[Category:1881 births]]\n",
      "[[Category:1955 deaths]]\n",
      "\n",
      "{{-}}\n",
      "{{bio-stub}}\n"
     ]
    }
   ],
   "source": [
    "# Running the Model on a page\n",
    "THRESHOLD = 0.95\n",
    "\n",
    "page_title = \"Fernand_Léger\"\n",
    "print(\"processing:\", page_title)\n",
    "print(\"\\n==========\\n\")\n",
    "result = process_page(page_title)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the crucial thing is to get the correct mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hyun Soong-jong': 1}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors['him']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Hyun Soong-jong', 0.99994767)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_links('Fernand_Léger','him',THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_addlink",
   "language": "python",
   "name": "venv_addlink"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
